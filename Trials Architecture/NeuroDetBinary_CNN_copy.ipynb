{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a696ffff",
   "metadata": {},
   "source": [
    "## NeuroDet CNN 2 CLass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55dd7d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import math\n",
    "\n",
    "\n",
    "# Disabling GPU for the moment because of the lack of the memory\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" \n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e041fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2870 files belonging to 2 classes.\n",
      "Found 394 files belonging to 2 classes.\n",
      "['no_tumor', 'tumor']\n",
      "['no_tumor', 'tumor']\n"
     ]
    }
   ],
   "source": [
    "train_path = 'BrainMRI_2Class/Training'\n",
    "test_path = 'BrainMRI_2Class/Testing'\n",
    "\n",
    "#train_glioma_dir = pathlib.Path(train_path + categories_path['glioma_tumor'])\n",
    "train_dir = pathlib.Path(train_path)\n",
    "test_dir = pathlib.Path(test_path)\n",
    "\n",
    "# Defining the parameters of the dataset\n",
    "batch_size = 32 #was 128 reduce to 16 to fit the data on the gpu ram\n",
    "img_height = 256 # was 128\n",
    "img_width = 256 # was 128\n",
    "\n",
    "# Loading the train dataset using keras.utils.image_dataset_from_directory\n",
    "# To use this method, please ensure you have tf.nigthly installed \n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(train_dir, seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size, color_mode = 'grayscale')\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(test_dir, seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size, color_mode = 'grayscale')\n",
    "\n",
    "print(train_ds.class_names)\n",
    "print(test_ds.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3632328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "print(len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5244a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got 80% acc at first but then removing the relu on the first dense layer helped push acc to 85%\n",
    "# 30 epochs seems to be instable, I moved to 35\n",
    "# changed previous class from 4 to 10. At 4, rerunning the model was producing high variance 73~85, even with 50epoch.\n",
    "class CNNTumorBin(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNTumorBin, self).__init__()\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "         #previous kernel filter number  that produced .75 was 10. 16 filter is not good\n",
    "        self.norm1_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.norm1_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.max1 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        \n",
    "        self.norm2_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.norm2_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.conv4 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.max2 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        self.norm3 = tf.keras.layers.BatchNormalization()\n",
    "        #self.conv5 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.max3 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        self.flat = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(80)\n",
    "        self.dense2 = tf.keras.layers.Dense(10, activation = 'relu')\n",
    "        self.out = tf.keras.layers.Dense(num_classes, activation = 'softmax')\n",
    "        \n",
    "    def call(self, batch_input, is_training=False):\n",
    "        c1 = self.conv1(batch_input)\n",
    "        c1 = tf.nn.relu(self.norm1_1(c1, training=is_training))\n",
    "        c2 = self.conv2(c1)\n",
    "        c2 = tf.nn.relu(self.norm1_2(c2, training=is_training))\n",
    "        m1 = self.max1(c2)\n",
    "        \n",
    "        #print(\"m_1 shape\")\n",
    "        #print(m1.shape)\n",
    "        \n",
    "        c3 = self.conv3(m1)\n",
    "        c3 = tf.nn.relu(self.norm2_1(c3, training=is_training))\n",
    "        c3 = self.conv3(c3)\n",
    "        c3 = tf.nn.relu(self.norm2_2(c3, training=is_training))\n",
    "        m2 = self.max2(c3)\n",
    "        \n",
    "        #print(\"m_2 shape\")\n",
    "        #print(m2.shape)\n",
    "        \n",
    "        c4 = self.conv4(m2)\n",
    "        c4 = tf.nn.relu(self.norm3(c4, training=is_training))\n",
    "        m3 = self.max3(c4)\n",
    "        \n",
    "        #print(\"m_3 shape\")\n",
    "        #print(m3.shape)\n",
    "        \n",
    "        # residual add\n",
    "        #resNet_add = batch_input + m3\n",
    "        \n",
    "        flat = self.flat(m3)\n",
    "        dense1 = self.dense1(flat)\n",
    "        desen2 = self.dense2(dense1)\n",
    "        \n",
    "        return self.out(desen2)\n",
    "    \n",
    "    def loss(self, probas, labels):\n",
    "        return tf.reduce_mean(tf.keras.metrics.sparse_categorical_crossentropy(labels, probas))\n",
    "    \n",
    "    def train(self, train_dataset):\n",
    "        list_loss = []\n",
    "        for images, labels in  train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                probas = self.call(images, is_training = True)\n",
    "                loss = self.loss(probas, labels)\n",
    "                acc = self.accuracy(probas, labels)\n",
    "                #print(\"Loss for the batch {}, accuracy {}\".format(loss, acc))\n",
    "                list_loss.append(loss)\n",
    "                \n",
    "            gradients = tape.gradient(loss, self.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        print(\"avg Loss for the epoch {}\".format(np.average(np.array(list_loss))))\n",
    "        \n",
    "\n",
    "    def test(self, test_dataset):\n",
    "        # maybe return all information to also output roc plots?\n",
    "        accs = []\n",
    "        for images, labels in test_dataset:\n",
    "            probas = self.call(images)\n",
    "            acc = self.accuracy(probas, labels)\n",
    "            accs.append(acc)\n",
    "            \n",
    "        return accs\n",
    "        \n",
    "    \n",
    "    def accuracy(self, probas, labels):\n",
    "        correct_predictions = tf.equal(tf.argmax(probas, 1), tf.cast(labels, tf.int64))\n",
    "        return tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8185bace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "avg Loss for the epoch 0.31639277935028076\n",
      "epoch 2\n",
      "avg Loss for the epoch 0.201790913939476\n",
      "epoch 3\n",
      "avg Loss for the epoch 0.17838150262832642\n",
      "epoch 4\n",
      "avg Loss for the epoch 0.1526583880186081\n",
      "epoch 5\n",
      "avg Loss for the epoch 0.12563054263591766\n",
      "epoch 6\n",
      "avg Loss for the epoch 0.12136561423540115\n",
      "epoch 7\n",
      "avg Loss for the epoch 0.14048883318901062\n",
      "epoch 8\n",
      "avg Loss for the epoch 0.11786632239818573\n",
      "epoch 9\n",
      "avg Loss for the epoch 0.11222001910209656\n",
      "epoch 10\n",
      "avg Loss for the epoch 0.10187883675098419\n",
      "epoch 11\n",
      "avg Loss for the epoch 0.12866449356079102\n",
      "epoch 12\n",
      "avg Loss for the epoch 0.12818990647792816\n",
      "epoch 13\n",
      "avg Loss for the epoch 0.09967473149299622\n",
      "epoch 14\n",
      "avg Loss for the epoch 0.09778390824794769\n",
      "epoch 15\n",
      "avg Loss for the epoch 0.08730661123991013\n",
      "epoch 16\n",
      "avg Loss for the epoch 0.09635381400585175\n",
      "epoch 17\n",
      "avg Loss for the epoch 0.09952192008495331\n",
      "epoch 18\n",
      "avg Loss for the epoch 0.07684851437807083\n",
      "epoch 19\n",
      "avg Loss for the epoch 0.07966846227645874\n",
      "epoch 20\n",
      "avg Loss for the epoch 0.07145946472883224\n",
      "epoch 21\n",
      "avg Loss for the epoch 0.07604923844337463\n",
      "epoch 22\n",
      "avg Loss for the epoch 0.07767990231513977\n",
      "epoch 23\n",
      "avg Loss for the epoch 0.07725405693054199\n",
      "epoch 24\n",
      "avg Loss for the epoch 0.06697379797697067\n",
      "epoch 25\n",
      "avg Loss for the epoch 0.0736340582370758\n",
      "epoch 26\n",
      "avg Loss for the epoch 0.07305800914764404\n",
      "epoch 27\n",
      "avg Loss for the epoch 0.061048757284879684\n",
      "epoch 28\n",
      "avg Loss for the epoch 0.06342072784900665\n",
      "epoch 29\n",
      "avg Loss for the epoch 0.055318683385849\n",
      "epoch 30\n",
      "avg Loss for the epoch 0.059843141585588455\n",
      "epoch 31\n",
      "avg Loss for the epoch 0.05013560131192207\n",
      "epoch 32\n",
      "avg Loss for the epoch 0.04740611091256142\n",
      "epoch 33\n",
      "avg Loss for the epoch 0.05734727159142494\n",
      "epoch 34\n",
      "avg Loss for the epoch 0.059892039746046066\n",
      "epoch 35\n",
      "avg Loss for the epoch 0.049613673239946365\n",
      "epoch 36\n",
      "avg Loss for the epoch 0.037410665303468704\n",
      "epoch 37\n",
      "avg Loss for the epoch 0.034825973212718964\n",
      "epoch 38\n",
      "avg Loss for the epoch 0.03616567701101303\n",
      "epoch 39\n",
      "avg Loss for the epoch 0.046331893652677536\n",
      "epoch 40\n",
      "avg Loss for the epoch 0.03351331129670143\n",
      "epoch 41\n",
      "avg Loss for the epoch 0.03927535191178322\n",
      "epoch 42\n",
      "avg Loss for the epoch 0.0328189954161644\n",
      "epoch 43\n",
      "avg Loss for the epoch 0.02991493046283722\n",
      "epoch 44\n",
      "avg Loss for the epoch 0.02607697993516922\n",
      "epoch 45\n",
      "avg Loss for the epoch 0.025754522532224655\n",
      "epoch 46\n",
      "avg Loss for the epoch 0.026491668075323105\n",
      "epoch 47\n",
      "avg Loss for the epoch 0.032257888466119766\n",
      "epoch 48\n",
      "avg Loss for the epoch 0.053482428193092346\n",
      "epoch 49\n",
      "avg Loss for the epoch 0.023405389860272408\n",
      "epoch 50\n",
      "avg Loss for the epoch 0.021947816014289856\n"
     ]
    }
   ],
   "source": [
    "cnn_tumorbin = CNNTumorBin(2)\n",
    "\n",
    "for i in range(50):\n",
    "    print(\"epoch \"+str(i+1))\n",
    "    cnn_tumorbin.train(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9f5df66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.82067305, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_mean(cnn_tumorbin.test(test_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396f8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

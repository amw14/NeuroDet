{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee680326",
   "metadata": {},
   "source": [
    "## NeuroDet version 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b06600",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6730f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import math\n",
    "\n",
    "\n",
    "# Disabling GPU for the moment because of the lack of the memory\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" \n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3727d09",
   "metadata": {},
   "source": [
    "### Loading the dataset of Brain Scan images\n",
    "Source of the Dataset: [Kaggle-Brain Tumor Classification](https://www.kaggle.com/sartajbhuvaji/brain-tumor-classification-mri?select=Testing)<br>\n",
    "Reference for operations performed : [Tensorflow tutorial: Load Images](https://www.tensorflow.org/tutorials/load_data/images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb78c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2870 files belonging to 4 classes.\n",
      "Found 394 files belonging to 4 classes.\n",
      "['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
      "['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n"
     ]
    }
   ],
   "source": [
    "categories_path = {'glioma_tumor': '/glioma_tumor', 'meningioma_tumor': '/meningioma_tumor', \n",
    "                   'pituitary_tumor': '/pituitary_tumor', 'no_tumor':'/no_tumor'}\n",
    "train_path = 'BrainMRI/Training'\n",
    "test_path = 'BrainMRI/Testing'\n",
    "\n",
    "#train_glioma_dir = pathlib.Path(train_path + categories_path['glioma_tumor'])\n",
    "train_dir = pathlib.Path(train_path)\n",
    "test_dir = pathlib.Path(test_path)\n",
    "\n",
    "# Defining the parameters of the dataset\n",
    "batch_size = 32 #was 128 reduce to 16 to fit the data on the gpu ram\n",
    "img_height = 256 # was 128\n",
    "img_width = 256 # was 128\n",
    "\n",
    "# Loading the train dataset using keras.utils.image_dataset_from_directory\n",
    "# To use this method, please ensure you have tf.nigthly installed \n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(train_dir, seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size, color_mode = 'grayscale')\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(test_dir, seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size, color_mode = 'grayscale')\n",
    "\n",
    "print(train_ds.class_names)\n",
    "print(test_ds.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d68bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0f72da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "augment_layer = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=1474)\n",
    "train_ds = train_ds.map(\n",
    "  lambda x, y: (augment_layer(x), y))\n",
    "print(len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a848065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c065980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at an instance of pituitary tumor from the dataset\n",
    "PIL.Image.open(list(train_dir.glob('pituitary_tumor/*.jpg'))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128e105",
   "metadata": {},
   "source": [
    "### Visualizing the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307473fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added np.squeeze to fix the dimension error on plt.imshow.\n",
    "# added cmap = 'gray' to specify that we are displaying grayscale images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(np.squeeze(images[i].numpy().astype(\"uint8\")),cmap='gray')\n",
    "        plt.title( train_ds.class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a278c301",
   "metadata": {},
   "source": [
    "### Normilizing the training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c0fc015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "print(len(train_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222aa509",
   "metadata": {},
   "source": [
    "### Implementing the CNN (based on ResNet)\n",
    "[tf.keras.layers.MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)<br>\n",
    "[tf.keras.layers.Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)<br>\n",
    "[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f1fc55",
   "metadata": {},
   "source": [
    "## Version 1 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71cf3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1 acheive 72% for 5 epoch --> 75% acc after 15 epoch\n",
    "class CNNResNet(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNResNet, self).__init__()\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "        self.norm = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.max1 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        \n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, activation='relu', padding =\"same\")\n",
    "        self.max2 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, activation='relu', padding =\"same\")\n",
    "        self.max3 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        self.flat = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(80, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(num_classes, activation = 'softmax')\n",
    "        \n",
    "    def call(self, batch_input):\n",
    "        c1 = self.conv1(batch_input)\n",
    "        c1 = tf.nn.relu(self.norm(c1))\n",
    "        m1 = self.max1(c1)\n",
    "        \n",
    "        #print(\"m_1 shape\")\n",
    "        #print(m1.shape)\n",
    "        \n",
    "        c2 = self.conv2(m1)\n",
    "        m2 = self.max2(c2)\n",
    "        \n",
    "        #print(\"m_2 shape\")\n",
    "        #print(m2.shape)\n",
    "        \n",
    "        c3 = self.conv2(m2)\n",
    "        m3 = self.max2(c3)\n",
    "        \n",
    "        #print(\"m_3 shape\")\n",
    "        #print(m3.shape)\n",
    "        \n",
    "        # residual add\n",
    "        #resNet_add = batch_input + m3\n",
    "        \n",
    "        flat = self.flat(m3)\n",
    "        dense = self.dense(flat)\n",
    "        \n",
    "        return self.out(dense)\n",
    "    \n",
    "    def loss(self, probas, labels):\n",
    "        return tf.reduce_mean(tf.keras.metrics.sparse_categorical_crossentropy(labels, probas))\n",
    "    \n",
    "    def train(self, train_dataset):\n",
    "        list_loss = []\n",
    "        for images, labels in  train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                probas = self.call(images)\n",
    "                loss = self.loss(probas, labels)\n",
    "                acc = self.accuracy(probas, labels)\n",
    "                #print(\"Loss for the batch {}, accuracy {}\".format(loss, acc))\n",
    "                list_loss.append(loss)\n",
    "                \n",
    "            gradients = tape.gradient(loss, self.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        print(\"avg Loss for the epoch {}\".format(np.average(np.array(list_loss))))\n",
    "        \n",
    "\n",
    "    def test(self, test_dataset):\n",
    "        # maybe return all information to also output roc plots?\n",
    "        accs = []\n",
    "        for images, labels in test_dataset:\n",
    "            probas = self.call(images)\n",
    "            acc = self.accuracy(probas, labels)\n",
    "            accs.append(acc)\n",
    "            \n",
    "        return accs\n",
    "        \n",
    "    \n",
    "    def accuracy(self, probas, labels):\n",
    "        correct_predictions = tf.equal(tf.argmax(probas, 1), tf.cast(labels, tf.int64))\n",
    "        return tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a715678e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "avg Loss for the epoch 0.9071904420852661\n",
      "epoch 1\n",
      "avg Loss for the epoch 0.48034706711769104\n",
      "epoch 2\n",
      "avg Loss for the epoch 0.30205032229423523\n",
      "epoch 3\n",
      "avg Loss for the epoch 0.1745443493127823\n",
      "epoch 4\n",
      "avg Loss for the epoch 0.10384365916252136\n",
      "epoch 5\n",
      "avg Loss for the epoch 0.07792734354734421\n",
      "epoch 6\n",
      "avg Loss for the epoch 0.02892071194946766\n",
      "epoch 7\n",
      "avg Loss for the epoch 0.027329551056027412\n",
      "epoch 8\n",
      "avg Loss for the epoch 0.013600606471300125\n",
      "epoch 9\n",
      "avg Loss for the epoch 0.06659140437841415\n",
      "epoch 10\n",
      "avg Loss for the epoch 0.10481758415699005\n",
      "epoch 11\n",
      "avg Loss for the epoch 0.019645199179649353\n",
      "epoch 12\n",
      "avg Loss for the epoch 0.0020508889574557543\n",
      "epoch 13\n",
      "avg Loss for the epoch 0.0006573863211087883\n",
      "epoch 14\n",
      "avg Loss for the epoch 0.00040436495328322053\n",
      "epoch 15\n",
      "avg Loss for the epoch 0.00011085155711043626\n",
      "epoch 16\n",
      "avg Loss for the epoch 8.348435221705586e-05\n",
      "epoch 17\n",
      "avg Loss for the epoch 6.727407890139148e-05\n",
      "epoch 18\n",
      "avg Loss for the epoch 5.794040407636203e-05\n",
      "epoch 19\n",
      "avg Loss for the epoch 4.959531725035049e-05\n",
      "epoch 20\n",
      "avg Loss for the epoch 4.338708458817564e-05\n",
      "epoch 21\n",
      "avg Loss for the epoch 3.807404937106185e-05\n",
      "epoch 22\n",
      "avg Loss for the epoch 3.4103679354302585e-05\n",
      "epoch 23\n",
      "avg Loss for the epoch 3.01888030662667e-05\n",
      "epoch 24\n",
      "avg Loss for the epoch 2.7214073270442896e-05\n",
      "epoch 25\n",
      "avg Loss for the epoch 2.4723722162889317e-05\n",
      "epoch 26\n",
      "avg Loss for the epoch 2.271707126055844e-05\n",
      "epoch 27\n",
      "avg Loss for the epoch 2.0502633560681716e-05\n",
      "epoch 28\n",
      "avg Loss for the epoch 1.8882385120377876e-05\n",
      "epoch 29\n",
      "avg Loss for the epoch 1.7238349755643867e-05\n"
     ]
    }
   ],
   "source": [
    "cnn_resNet = CNNResNet(4)\n",
    "\n",
    "for i in range(30):\n",
    "    print(\"epoch \"+str(i))\n",
    "    cnn_resNet.train(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30180e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.73221153, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_mean(cnn_resNet.test(test_ds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd353c93",
   "metadata": {},
   "source": [
    "## Version 2 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691da5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2: Achieve 75% accuracy on testing set for 20 epoch\n",
    "class CNNResNet(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNResNet, self).__init__()\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "        \n",
    "         #previous kernel filter number  that produced .75 was 10. 16 filter is not good\n",
    "        self.norm = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.max1 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        \n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, activation='relu', padding =\"same\")\n",
    "        self.conv4 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, activation='relu', padding =\"same\")\n",
    "        self.max2 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        self.conv5 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, activation='relu', padding =\"same\")\n",
    "        self.max3 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        self.flat = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(80, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(num_classes, activation = 'softmax')\n",
    "        \n",
    "    def call(self, batch_input):\n",
    "        c1 = self.conv1(batch_input)\n",
    "        c2 = self.conv2(c1)\n",
    "        c2 = tf.nn.relu(self.norm(c2))\n",
    "        m1 = self.max1(c2)\n",
    "        \n",
    "        #print(\"m_1 shape\")\n",
    "        #print(m1.shape)\n",
    "        \n",
    "        c3 = self.conv3(m1)\n",
    "        c4 = self.conv3(c3)\n",
    "        m2 = self.max2(c4)\n",
    "        \n",
    "        #print(\"m_2 shape\")\n",
    "        #print(m2.shape)\n",
    "        \n",
    "        c5 = self.conv4(m2)\n",
    "        m3 = self.max2(c5)\n",
    "        \n",
    "        #print(\"m_3 shape\")\n",
    "        #print(m3.shape)\n",
    "        \n",
    "        # residual add\n",
    "        #resNet_add = batch_input + m3\n",
    "        \n",
    "        flat = self.flat(m3)\n",
    "        dense = self.dense(flat)\n",
    "        \n",
    "        return self.out(dense)\n",
    "    \n",
    "    def loss(self, probas, labels):\n",
    "        return tf.reduce_mean(tf.keras.metrics.sparse_categorical_crossentropy(labels, probas))\n",
    "    \n",
    "    def train(self, train_dataset):\n",
    "        list_loss = []\n",
    "        for images, labels in  train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                probas = self.call(images)\n",
    "                loss = self.loss(probas, labels)\n",
    "                acc = self.accuracy(probas, labels)\n",
    "                #print(\"Loss for the batch {}, accuracy {}\".format(loss, acc))\n",
    "                list_loss.append(loss)\n",
    "                \n",
    "            gradients = tape.gradient(loss, self.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        print(\"avg Loss for the epoch {}\".format(np.average(np.array(list_loss))))\n",
    "        \n",
    "\n",
    "    def test(self, test_dataset):\n",
    "        # maybe return all information to also output roc plots?\n",
    "        accs = []\n",
    "        for images, labels in test_dataset:\n",
    "            probas = self.call(images)\n",
    "            acc = self.accuracy(probas, labels)\n",
    "            accs.append(acc)\n",
    "            \n",
    "        return accs\n",
    "        \n",
    "    \n",
    "    def accuracy(self, probas, labels):\n",
    "        correct_predictions = tf.equal(tf.argmax(probas, 1), tf.cast(labels, tf.int64))\n",
    "        return tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_resNet = CNNResNet(4)\n",
    "\n",
    "for i in range(25):\n",
    "    print(\"epoch \"+str(i+1))\n",
    "    cnn_resNet.train(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.reduce_mean(cnn_resNet.test(test_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23eb44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75e405d1",
   "metadata": {},
   "source": [
    "## Version3 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dce1edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 3: \n",
    "class CNNResNet(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNResNet, self).__init__()\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "         #previous kernel filter number  that produced .75 was 10. 16 filter is not good\n",
    "        self.norm1_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.norm1_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.max1 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        \n",
    "        self.norm2_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.norm2_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.conv4 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.max2 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        self.norm3 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv5 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.max3 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        self.flat = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(80, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(num_classes, activation = 'softmax')\n",
    "        \n",
    "    def call(self, batch_input, is_training=False):\n",
    "        c1 = self.conv1(batch_input)\n",
    "        c1 = tf.nn.relu(self.norm1_1(c1, training=is_training))\n",
    "        c2 = self.conv2(c1)\n",
    "        c2 = tf.nn.relu(self.norm1_2(c2, training=is_training))\n",
    "        m1 = self.max1(c2)\n",
    "        \n",
    "        #print(\"m_1 shape\")\n",
    "        #print(m1.shape)\n",
    "        \n",
    "        c3 = self.conv3(m1)\n",
    "        c3 = tf.nn.relu(self.norm2_1(c3, training=is_training))\n",
    "        c4 = self.conv3(c3)\n",
    "        c4 = tf.nn.relu(self.norm2_2(c4, training=is_training))\n",
    "        m2 = self.max2(c4)\n",
    "        \n",
    "        #print(\"m_2 shape\")\n",
    "        #print(m2.shape)\n",
    "        \n",
    "        c5 = self.conv4(m2)\n",
    "        c5 = tf.nn.relu(self.norm3(c5, training=is_training))\n",
    "        m3 = self.max2(c5)\n",
    "        \n",
    "        #print(\"m_3 shape\")\n",
    "        #print(m3.shape)\n",
    "        \n",
    "        # residual add\n",
    "        #resNet_add = batch_input + m3\n",
    "        \n",
    "        flat = self.flat(m3)\n",
    "        dense = self.dense(flat)\n",
    "        \n",
    "        return self.out(dense)\n",
    "    \n",
    "    def loss(self, probas, labels):\n",
    "        return tf.reduce_mean(tf.keras.metrics.sparse_categorical_crossentropy(labels, probas))\n",
    "    \n",
    "    def train(self, train_dataset):\n",
    "        list_loss = []\n",
    "        for images, labels in  train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                probas = self.call(images, is_training = True)\n",
    "                loss = self.loss(probas, labels)\n",
    "                acc = self.accuracy(probas, labels)\n",
    "                #print(\"Loss for the batch {}, accuracy {}\".format(loss, acc))\n",
    "                list_loss.append(loss)\n",
    "                \n",
    "            gradients = tape.gradient(loss, self.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        print(\"avg Loss for the epoch {}\".format(np.average(np.array(list_loss))))\n",
    "        \n",
    "\n",
    "    def test(self, test_dataset):\n",
    "        # maybe return all information to also output roc plots?\n",
    "        accs = []\n",
    "        for images, labels in test_dataset:\n",
    "            probas = self.call(images)\n",
    "            acc = self.accuracy(probas, labels)\n",
    "            accs.append(acc)\n",
    "            \n",
    "        return accs\n",
    "        \n",
    "    \n",
    "    def accuracy(self, probas, labels):\n",
    "        correct_predictions = tf.equal(tf.argmax(probas, 1), tf.cast(labels, tf.int64))\n",
    "        return tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a74ebaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "avg Loss for the epoch 1.282031774520874\n",
      "epoch 2\n",
      "avg Loss for the epoch 0.45946231484413147\n",
      "epoch 3\n",
      "avg Loss for the epoch 0.28875723481178284\n",
      "epoch 4\n",
      "avg Loss for the epoch 0.22214679419994354\n",
      "epoch 5\n",
      "avg Loss for the epoch 0.13820815086364746\n",
      "epoch 6\n",
      "avg Loss for the epoch 0.07370144873857498\n",
      "epoch 7\n",
      "avg Loss for the epoch 0.047317326068878174\n",
      "epoch 8\n",
      "avg Loss for the epoch 0.046539660543203354\n",
      "epoch 9\n",
      "avg Loss for the epoch 0.02806546725332737\n",
      "epoch 10\n",
      "avg Loss for the epoch 0.016650326550006866\n",
      "epoch 11\n",
      "avg Loss for the epoch 0.011769254691898823\n",
      "epoch 12\n",
      "avg Loss for the epoch 0.0100216930732131\n",
      "epoch 13\n",
      "avg Loss for the epoch 0.006853950675576925\n",
      "epoch 14\n",
      "avg Loss for the epoch 0.0040313471108675\n",
      "epoch 15\n",
      "avg Loss for the epoch 0.003794342279434204\n",
      "epoch 16\n",
      "avg Loss for the epoch 0.003463100641965866\n",
      "epoch 17\n",
      "avg Loss for the epoch 0.0033578325528651476\n",
      "epoch 18\n",
      "avg Loss for the epoch 0.0020669796504080296\n",
      "epoch 19\n",
      "avg Loss for the epoch 0.0018607914680615067\n",
      "epoch 20\n",
      "avg Loss for the epoch 0.0015804159920662642\n",
      "epoch 21\n",
      "avg Loss for the epoch 0.001334062428213656\n",
      "epoch 22\n",
      "avg Loss for the epoch 0.0012294808402657509\n",
      "epoch 23\n",
      "avg Loss for the epoch 0.0011347070103511214\n",
      "epoch 24\n",
      "avg Loss for the epoch 0.0010040849447250366\n",
      "epoch 25\n",
      "avg Loss for the epoch 0.0008449899614788592\n",
      "epoch 26\n",
      "avg Loss for the epoch 0.0008435702766291797\n",
      "epoch 27\n",
      "avg Loss for the epoch 0.001032889587804675\n",
      "epoch 28\n",
      "avg Loss for the epoch 0.0006812324863858521\n",
      "epoch 29\n",
      "avg Loss for the epoch 0.0005465222639031708\n",
      "epoch 30\n",
      "avg Loss for the epoch 0.0005573664675466716\n"
     ]
    }
   ],
   "source": [
    "cnn_resNet = CNNResNet(4)\n",
    "\n",
    "for i in range(30):\n",
    "    print(\"epoch \"+str(i+1))\n",
    "    cnn_resNet.train(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc741ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.7807692, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_mean(cnn_resNet.test(test_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc15c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 3: \n",
    "class CNNResNet(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNResNet, self).__init__()\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "         #previous kernel filter number  that produced .75 was 10. 16 filter is not good\n",
    "        self.norm1_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.norm1_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.max1 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        \n",
    "        self.norm2_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.norm2_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.conv4 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.max2 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        self.norm3 = tf.keras.layers.BatchNormalization()\n",
    "        #self.conv5 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.max3 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        self.flat = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(80, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(num_classes, activation = 'softmax')\n",
    "        \n",
    "    def call(self, batch_input, is_training=False):\n",
    "        c1 = self.conv1(batch_input)\n",
    "        c1 = tf.nn.relu(self.norm1_1(c1, training=is_training))\n",
    "        c2 = self.conv2(c1)\n",
    "        c2 = tf.nn.relu(self.norm1_2(c2, training=is_training))\n",
    "        m1 = self.max1(c2)\n",
    "        \n",
    "        #print(\"m_1 shape\")\n",
    "        #print(m1.shape)\n",
    "        \n",
    "        c3 = self.conv3(m1)\n",
    "        c3 = tf.nn.relu(self.norm2_1(c3, training=is_training))\n",
    "        c3 = self.conv3(c3)\n",
    "        c3 = tf.nn.relu(self.norm2_2(c3, training=is_training))\n",
    "        m2 = self.max2(c3)\n",
    "        \n",
    "        #print(\"m_2 shape\")\n",
    "        #print(m2.shape)\n",
    "        \n",
    "        c4 = self.conv4(m2)\n",
    "        c4 = tf.nn.relu(self.norm3(c4, training=is_training))\n",
    "        m3 = self.max2(c4)\n",
    "        \n",
    "        #print(\"m_3 shape\")\n",
    "        #print(m3.shape)\n",
    "        \n",
    "        # residual add\n",
    "        #resNet_add = batch_input + m3\n",
    "        \n",
    "        flat = self.flat(m3)\n",
    "        dense = self.dense(flat)\n",
    "        \n",
    "        return self.out(dense)\n",
    "    \n",
    "    def loss(self, probas, labels):\n",
    "        return tf.reduce_mean(tf.keras.metrics.sparse_categorical_crossentropy(labels, probas))\n",
    "    \n",
    "    def train(self, train_dataset):\n",
    "        list_loss = []\n",
    "        for images, labels in  train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                probas = self.call(images, is_training = True)\n",
    "                loss = self.loss(probas, labels)\n",
    "                acc = self.accuracy(probas, labels)\n",
    "                #print(\"Loss for the batch {}, accuracy {}\".format(loss, acc))\n",
    "                list_loss.append(loss)\n",
    "                \n",
    "            gradients = tape.gradient(loss, self.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        print(\"avg Loss for the epoch {}\".format(np.average(np.array(list_loss))))\n",
    "        \n",
    "\n",
    "    def test(self, test_dataset):\n",
    "        # maybe return all information to also output roc plots?\n",
    "        accs = []\n",
    "        for images, labels in test_dataset:\n",
    "            probas = self.call(images)\n",
    "            acc = self.accuracy(probas, labels)\n",
    "            accs.append(acc)\n",
    "            \n",
    "        return accs\n",
    "        \n",
    "    \n",
    "    def accuracy(self, probas, labels):\n",
    "        correct_predictions = tf.equal(tf.argmax(probas, 1), tf.cast(labels, tf.int64))\n",
    "        return tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b027b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "avg Loss for the epoch 1.4919942617416382\n",
      "epoch 2\n",
      "avg Loss for the epoch 0.5002593994140625\n",
      "epoch 3\n",
      "avg Loss for the epoch 0.3394598662853241\n",
      "epoch 4\n",
      "avg Loss for the epoch 0.22890014946460724\n",
      "epoch 5\n",
      "avg Loss for the epoch 0.16161759197711945\n",
      "epoch 6\n",
      "avg Loss for the epoch 0.10571421682834625\n",
      "epoch 7\n",
      "avg Loss for the epoch 0.07988719642162323\n",
      "epoch 8\n",
      "avg Loss for the epoch 0.058037079870700836\n",
      "epoch 9\n",
      "avg Loss for the epoch 0.06113129109144211\n",
      "epoch 10\n",
      "avg Loss for the epoch 0.05121428146958351\n",
      "epoch 11\n",
      "avg Loss for the epoch 0.022130224853754044\n",
      "epoch 12\n",
      "avg Loss for the epoch 0.011321836151182652\n",
      "epoch 13\n",
      "avg Loss for the epoch 0.006839027162641287\n",
      "epoch 14\n",
      "avg Loss for the epoch 0.0034706571605056524\n",
      "epoch 15\n",
      "avg Loss for the epoch 0.0029842592775821686\n",
      "epoch 16\n",
      "avg Loss for the epoch 0.002776384586468339\n",
      "epoch 17\n",
      "avg Loss for the epoch 0.001994882244616747\n",
      "epoch 18\n",
      "avg Loss for the epoch 0.0018058548448607326\n",
      "epoch 19\n",
      "avg Loss for the epoch 0.0023045078851282597\n",
      "epoch 20\n",
      "avg Loss for the epoch 0.0015170241240411997\n",
      "epoch 21\n",
      "avg Loss for the epoch 0.0010806246427819133\n",
      "epoch 22\n",
      "avg Loss for the epoch 0.001028236118145287\n",
      "epoch 23\n",
      "avg Loss for the epoch 0.0010953196324408054\n",
      "epoch 24\n",
      "avg Loss for the epoch 0.0011648766230791807\n",
      "epoch 25\n",
      "avg Loss for the epoch 0.0008365495596081018\n",
      "epoch 26\n",
      "avg Loss for the epoch 0.0007190612377598882\n",
      "epoch 27\n",
      "avg Loss for the epoch 0.0006616777973249555\n",
      "epoch 28\n",
      "avg Loss for the epoch 0.0005528100300580263\n",
      "epoch 29\n",
      "avg Loss for the epoch 0.00048451294424012303\n",
      "epoch 30\n",
      "avg Loss for the epoch 0.0004655412049032748\n"
     ]
    }
   ],
   "source": [
    "cnn_resNet = CNNResNet(4)\n",
    "\n",
    "for i in range(30):\n",
    "    print(\"epoch \"+str(i+1))\n",
    "    cnn_resNet.train(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9210aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.7711538, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_mean(cnn_resNet.test(test_ds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319dd766",
   "metadata": {},
   "source": [
    "## Version 4 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea50396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 4: \n",
    "class CNNResNet(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNResNet, self).__init__()\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "         #previous kernel filter number  that produced .75 was 10. 16 filter is not good\n",
    "        self.norm1_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.norm1_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.max1 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        \n",
    "        self.norm2_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.norm2_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.conv4 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.max2 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        self.norm3_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.norm3_2 = tf.keras.layers.BatchNormalization()\n",
    "        #self.conv5 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        #self.conv6 = tf.keras.layers.Conv2D(filters=10, kernel_size=3, padding =\"same\")\n",
    "        self.max3 = tf.keras.layers.MaxPooling2D(padding =\"same\")\n",
    "        \n",
    "        self.flat = tf.keras.layers.Flatten()\n",
    "        #self.dense1 = tf.keras.layers.Dense(190)\n",
    "        #self.drop = tf.keras.layers.Dropout(0.25)\n",
    "        self.dense2 = tf.keras.layers.Dense(80, activation = 'relu')\n",
    "        self.out = tf.keras.layers.Dense(num_classes, activation = 'softmax')\n",
    "        \n",
    "    def call(self, batch_input, is_training=False):\n",
    "        c1 = self.conv1(batch_input)\n",
    "        c1 = tf.nn.relu(self.norm1_1(c1, training=is_training))\n",
    "        c2 = self.conv2(c1)\n",
    "        c2 = tf.nn.relu(self.norm1_2(c2, training=is_training))\n",
    "        m1 = self.max1(c2)\n",
    "        \n",
    "        #print(\"m_1 shape\")\n",
    "        #print(m1.shape)\n",
    "        \n",
    "        c3 = self.conv3(m1)\n",
    "        c3 = tf.nn.relu(self.norm2_1(c3, training=is_training))\n",
    "        c3 = self.conv3(c3)\n",
    "        c3 = tf.nn.relu(self.norm2_2(c3, training=is_training))\n",
    "        m2 = self.max2(c3)\n",
    "        \n",
    "        #print(\"m_2 shape\")\n",
    "        #print(m2.shape)\n",
    "        \n",
    "        c4 = self.conv4(m2)\n",
    "        c4 = tf.nn.relu(self.norm3_1(c4, training=is_training))\n",
    "        m3 = self.max3(c4)\n",
    "        \n",
    "        #print(\"m_3 shape\")\n",
    "        #print(m3.shape)\n",
    "        \n",
    "        # residual add\n",
    "        #resNet_add = batch_input + m3\n",
    "        \n",
    "        flat = self.flat(m3)\n",
    "        #dense1 = self.dense1(flat)\n",
    "        #dense1 = self.drop(dense1, training=is_training)\n",
    "        dense2 = self.dense2(flat)\n",
    "        \n",
    "        return self.out(dense2)\n",
    "    \n",
    "    def loss(self, probas, labels):\n",
    "        return tf.reduce_mean(tf.keras.metrics.sparse_categorical_crossentropy(labels, probas))\n",
    "    \n",
    "    def train(self, train_dataset):\n",
    "        list_loss = []\n",
    "        for images, labels in  train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                probas = self.call(images, is_training = True)\n",
    "                loss = self.loss(probas, labels)\n",
    "                acc = self.accuracy(probas, labels)\n",
    "                #print(\"Loss for the batch {}, accuracy {}\".format(loss, acc))\n",
    "                list_loss.append(loss)\n",
    "                \n",
    "            gradients = tape.gradient(loss, self.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        print(\"avg Loss for the epoch {}\".format(np.average(np.array(list_loss))))\n",
    "        \n",
    "\n",
    "    def test(self, test_dataset):\n",
    "        # maybe return all information to also output roc plots?\n",
    "        accs = []\n",
    "        for images, labels in test_dataset:\n",
    "            probas = self.call(images)\n",
    "            acc = self.accuracy(probas, labels)\n",
    "            accs.append(acc)\n",
    "            \n",
    "        return accs\n",
    "        \n",
    "    \n",
    "    def accuracy(self, probas, labels):\n",
    "        correct_predictions = tf.equal(tf.argmax(probas, 1), tf.cast(labels, tf.int64))\n",
    "        return tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f30936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "avg Loss for the epoch 1.278624415397644\n",
      "epoch 2\n",
      "avg Loss for the epoch 0.762274444103241\n",
      "epoch 3\n",
      "avg Loss for the epoch 0.6179163455963135\n",
      "epoch 4\n",
      "avg Loss for the epoch 0.5245347023010254\n",
      "epoch 5\n",
      "avg Loss for the epoch 0.46221864223480225\n",
      "epoch 6\n",
      "avg Loss for the epoch 0.4074830412864685\n",
      "epoch 7\n",
      "avg Loss for the epoch 0.37502697110176086\n",
      "epoch 8\n",
      "avg Loss for the epoch 0.32681527733802795\n",
      "epoch 9\n",
      "avg Loss for the epoch 0.3376762270927429\n",
      "epoch 10\n",
      "avg Loss for the epoch 0.2933810353279114\n",
      "epoch 11\n",
      "avg Loss for the epoch 0.28921476006507874\n",
      "epoch 12\n",
      "avg Loss for the epoch 0.25952664017677307\n",
      "epoch 13\n",
      "avg Loss for the epoch 0.25908124446868896\n",
      "epoch 14\n",
      "avg Loss for the epoch 0.21469993889331818\n",
      "epoch 15\n",
      "avg Loss for the epoch 0.21240085363388062\n",
      "epoch 16\n",
      "avg Loss for the epoch 0.2005583643913269\n",
      "epoch 17\n",
      "avg Loss for the epoch 0.19345201551914215\n",
      "epoch 18\n",
      "avg Loss for the epoch 0.1690511852502823\n",
      "epoch 19\n",
      "avg Loss for the epoch 0.18721158802509308\n",
      "epoch 20\n",
      "avg Loss for the epoch 0.16996634006500244\n",
      "epoch 21\n",
      "avg Loss for the epoch 0.1419081687927246\n",
      "epoch 22\n",
      "avg Loss for the epoch 0.11779950559139252\n",
      "epoch 23\n",
      "avg Loss for the epoch 0.11010352522134781\n",
      "epoch 24\n",
      "avg Loss for the epoch 0.12197153270244598\n",
      "epoch 25\n",
      "avg Loss for the epoch 0.1345328390598297\n",
      "epoch 26\n",
      "avg Loss for the epoch 0.1267322301864624\n",
      "epoch 27\n",
      "avg Loss for the epoch 0.09705472737550735\n",
      "epoch 28\n",
      "avg Loss for the epoch 0.07751500606536865\n",
      "epoch 29\n",
      "avg Loss for the epoch 0.09062658250331879\n",
      "epoch 30\n",
      "avg Loss for the epoch 0.09181411564350128\n",
      "epoch 31\n",
      "avg Loss for the epoch 0.09330528229475021\n",
      "epoch 32\n",
      "avg Loss for the epoch 0.08008754253387451\n",
      "epoch 33\n",
      "avg Loss for the epoch 0.09468939155340195\n",
      "epoch 34\n",
      "avg Loss for the epoch 0.07278799265623093\n",
      "epoch 35\n",
      "avg Loss for the epoch 0.059476450085639954\n",
      "epoch 36\n",
      "avg Loss for the epoch 0.062340255826711655\n",
      "epoch 37\n",
      "avg Loss for the epoch 0.09463907778263092\n",
      "epoch 38\n",
      "avg Loss for the epoch 0.0880553275346756\n",
      "epoch 39\n",
      "avg Loss for the epoch 0.06063055619597435\n",
      "epoch 40\n",
      "avg Loss for the epoch 0.05322263017296791\n"
     ]
    }
   ],
   "source": [
    "cnn_resNet = CNNResNet(4)\n",
    "\n",
    "for i in range(40):\n",
    "    print(\"epoch \"+str(i+1))\n",
    "    cnn_resNet.train(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1918fe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.4846154, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_mean(cnn_resNet.test(test_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad6acb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

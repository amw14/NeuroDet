{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee680326",
   "metadata": {},
   "source": [
    "## NeuroDet: CNN-LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b06600",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6730f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import math\n",
    "\n",
    "# Disabling GPU for the moment because of the lack of the memory\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" \n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3727d09",
   "metadata": {},
   "source": [
    "### Loading the dataset of Brain Scan images\n",
    "#### Full data for training and testing\n",
    "Source of the Dataset: [Kaggle-Brain Tumor Classification](https://www.kaggle.com/sartajbhuvaji/brain-tumor-classification-mri?select=Testing)<br>\n",
    "Reference for operations performed : [Tensorflow tutorial: Load Images](https://www.tensorflow.org/tutorials/load_data/images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb78c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of train examples: 2870\n",
      "Total number of test examples: 394\n"
     ]
    }
   ],
   "source": [
    "categories_path = {'glioma_tumor': '/glioma_tumor', 'meningioma_tumor': '/meningioma_tumor', \n",
    "                   'pituitary_tumor': '/pituitary_tumor', 'no_tumor':'/no_tumor'}\n",
    "train_path = 'BrainMRI/Training'\n",
    "test_path = 'BrainMRI/Testing'\n",
    "\n",
    "# train_glioma_dir = pathlib.Path(train_path + categories_path['glioma_tumor'])\n",
    "train_dir = pathlib.Path(train_path)\n",
    "test_dir = pathlib.Path(test_path)\n",
    "\n",
    "# Training data: number of examples\n",
    "# label = 0 | glioma_tumor: 826\n",
    "# label = 1 | meningioma_tumor: 822\n",
    "# label = 2 | no_tumor: 395 \n",
    "# label = 3 | pituitary_tumor: 827 \n",
    "\n",
    "# Testing data: number of examples\n",
    "# label = 0 | glioma_tumor: 100\n",
    "# label = 1 | meningioma_tumor: 115\n",
    "# label = 2 | no_tumor: 105\n",
    "# label = 3 | pituitary_tumor: 74\n",
    "\n",
    "num_train_examples_dict = {\n",
    "    \"label_0\": 826, \"label_1\": 822, \"label_2\": 395, \"label_3\": 827}\n",
    "\n",
    "num_test_examples_dict = {\n",
    "    \"label_0\": 100, \"label_1\": 115, \"label_2\": 105, \"label_3\": 74}\n",
    "\n",
    "\n",
    "num_train_examples = sum(num_train_examples_dict.values()) # 2870\n",
    "num_test_examples = sum(num_test_examples_dict.values()) # 394\n",
    "\n",
    "print(f'Total number of train examples: {num_train_examples}')\n",
    "print(f'Total number of test examples: {num_test_examples}') \n",
    "\n",
    "# Defining the parameters of the dataset\n",
    "batch_size = 128\n",
    "img_height = 256\n",
    "img_width = 256 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d1bc6b",
   "metadata": {},
   "source": [
    "#### 4-Class Classification Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a46cbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2870 files belonging to 4 classes.\n",
      "Found 394 files belonging to 4 classes.\n",
      "['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
      "['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n"
     ]
    }
   ],
   "source": [
    "# Loading the train dataset using keras.utils.image_dataset_from_directory\n",
    "# To use this method, please ensure you have tf.nigthly installed \n",
    "train_data_full = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir, seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=num_train_examples, \n",
    "    shuffle=True,\n",
    "    color_mode = 'grayscale')\n",
    "\n",
    "test_data_full = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir, seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=num_test_examples, \n",
    "    shuffle=True,\n",
    "    color_mode = 'grayscale')\n",
    "\n",
    "print(train_data_full.class_names)\n",
    "print(test_data_full.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41bb352",
   "metadata": {},
   "source": [
    "#### 2-Class Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74113712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2870 files belonging to 2 classes.\n",
      "Found 394 files belonging to 2 classes.\n",
      "['no_tumor', 'tumor']\n",
      "['no_tumor', 'tumor']\n"
     ]
    }
   ],
   "source": [
    "train2_path = 'BrainMRI_2Class/Training'\n",
    "test2_path = 'BrainMRI_2Class/Testing'\n",
    "\n",
    "train2_dir = pathlib.Path(train2_path)\n",
    "test2_dir = pathlib.Path(test2_path)\n",
    "\n",
    "# Loading the train dataset using keras.utils.image_dataset_from_directory\n",
    "# To use this method, please ensure you have tf.nigthly installed \n",
    "train2_data_full = tf.keras.utils.image_dataset_from_directory(\n",
    "    train2_dir, seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=num_train_examples, \n",
    "    color_mode = 'grayscale')\n",
    "\n",
    "test2_data_full = tf.keras.utils.image_dataset_from_directory(\n",
    "    test2_dir, seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=num_test_examples, \n",
    "    color_mode = 'grayscale')\n",
    "\n",
    "print(train2_data_full.class_names)\n",
    "print(test2_data_full.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a278c301",
   "metadata": {},
   "source": [
    "### Normalizing the training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0fc015",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "train_data_full = train_data_full.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_data_full = test_data_full.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "train2_data_full = train2_data_full.map(lambda x, y: (normalization_layer(x), y))\n",
    "test2_data_full = test2_data_full.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d3c646",
   "metadata": {},
   "source": [
    "### Separate data into images and labels for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ed79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_data_full:\n",
    "    x_train = images\n",
    "    y_train = labels\n",
    "    \n",
    "for images, labels in test_data_full:\n",
    "    x_test = images\n",
    "    y_test = labels\n",
    "    \n",
    "for images, labels in train2_data_full:\n",
    "    x_train2 = images\n",
    "    y_train2 = labels\n",
    "    \n",
    "for images, labels in test2_data_full:\n",
    "    x_test2 = images\n",
    "    y_test2 = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82d3ff",
   "metadata": {},
   "source": [
    "# 4-Class CNN-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d5a0330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = (2870, 256, 256, 1)\n",
      "Epoch 1/50\n",
      "90/90 [==============================] - 102s 1s/step - loss: 1.0746 - acc: 0.5380\n",
      "Epoch 2/50\n",
      "90/90 [==============================] - 100s 1s/step - loss: 0.7799 - acc: 0.6735\n",
      "Epoch 3/50\n",
      "90/90 [==============================] - 104s 1s/step - loss: 0.6984 - acc: 0.7084\n",
      "Epoch 4/50\n",
      "90/90 [==============================] - 102s 1s/step - loss: 0.5937 - acc: 0.7481\n",
      "Epoch 5/50\n",
      "90/90 [==============================] - 100s 1s/step - loss: 0.5266 - acc: 0.7829\n",
      "Epoch 6/50\n",
      "90/90 [==============================] - 101s 1s/step - loss: 0.5117 - acc: 0.7878\n",
      "Epoch 7/50\n",
      "90/90 [==============================] - 104s 1s/step - loss: 0.4510 - acc: 0.8195\n",
      "Epoch 8/50\n",
      "90/90 [==============================] - 102s 1s/step - loss: 0.3794 - acc: 0.8411\n",
      "Epoch 9/50\n",
      "90/90 [==============================] - 102s 1s/step - loss: 0.2994 - acc: 0.8774\n",
      "Epoch 10/50\n",
      "90/90 [==============================] - 103s 1s/step - loss: 0.2789 - acc: 0.8836\n",
      "Epoch 11/50\n",
      "90/90 [==============================] - 104s 1s/step - loss: 0.3711 - acc: 0.8571\n",
      "Epoch 12/50\n",
      "90/90 [==============================] - 100s 1s/step - loss: 0.2773 - acc: 0.8948\n",
      "Epoch 13/50\n",
      "90/90 [==============================] - 96s 1s/step - loss: 0.2227 - acc: 0.9174\n",
      "Epoch 14/50\n",
      "90/90 [==============================] - 95s 1s/step - loss: 0.1828 - acc: 0.9307\n",
      "Epoch 15/50\n",
      "90/90 [==============================] - 95s 1s/step - loss: 0.1959 - acc: 0.9286\n",
      "Epoch 16/50\n",
      "90/90 [==============================] - 96s 1s/step - loss: 0.1443 - acc: 0.9467\n",
      "Epoch 17/50\n",
      "90/90 [==============================] - 95s 1s/step - loss: 0.1336 - acc: 0.9523\n",
      "Epoch 18/50\n",
      "90/90 [==============================] - 95s 1s/step - loss: 0.1575 - acc: 0.9460\n",
      "Epoch 19/50\n",
      "90/90 [==============================] - 95s 1s/step - loss: 0.1559 - acc: 0.9387\n",
      "Epoch 20/50\n",
      "90/90 [==============================] - 130s 1s/step - loss: 0.1374 - acc: 0.9495\n",
      "Epoch 21/50\n",
      "90/90 [==============================] - 114s 1s/step - loss: 0.1149 - acc: 0.9634\n",
      "Epoch 22/50\n",
      "90/90 [==============================] - 111s 1s/step - loss: 0.0726 - acc: 0.9760\n",
      "Epoch 23/50\n",
      "90/90 [==============================] - 111s 1s/step - loss: 0.0560 - acc: 0.9794\n",
      "Epoch 24/50\n",
      "90/90 [==============================] - 113s 1s/step - loss: 0.0698 - acc: 0.9777\n",
      "Epoch 25/50\n",
      "90/90 [==============================] - 107s 1s/step - loss: 0.0777 - acc: 0.9763\n",
      "Epoch 26/50\n",
      "90/90 [==============================] - 102s 1s/step - loss: 0.0730 - acc: 0.9787\n",
      "Epoch 27/50\n",
      "90/90 [==============================] - 104s 1s/step - loss: 0.0632 - acc: 0.9787\n",
      "Epoch 28/50\n",
      "90/90 [==============================] - 101s 1s/step - loss: 0.0947 - acc: 0.9683\n",
      "Epoch 29/50\n",
      "90/90 [==============================] - 100s 1s/step - loss: 0.0446 - acc: 0.9836\n",
      "Epoch 30/50\n",
      "90/90 [==============================] - 101s 1s/step - loss: 0.0633 - acc: 0.9829\n",
      "Epoch 31/50\n",
      "90/90 [==============================] - 100s 1s/step - loss: 0.0554 - acc: 0.9826\n",
      "Epoch 32/50\n",
      "90/90 [==============================] - 95s 1s/step - loss: 0.0598 - acc: 0.9808\n",
      "Epoch 33/50\n",
      "90/90 [==============================] - 107s 1s/step - loss: 0.1660 - acc: 0.9446\n",
      "Epoch 34/50\n",
      "90/90 [==============================] - 102s 1s/step - loss: 0.0951 - acc: 0.9672\n",
      "Epoch 35/50\n",
      "90/90 [==============================] - 101s 1s/step - loss: 0.0375 - acc: 0.9871\n",
      "Epoch 36/50\n",
      "90/90 [==============================] - 103s 1s/step - loss: 0.0474 - acc: 0.9871\n",
      "Epoch 37/50\n",
      "90/90 [==============================] - 106s 1s/step - loss: 0.0564 - acc: 0.9805\n",
      "Epoch 38/50\n",
      "90/90 [==============================] - 123s 1s/step - loss: 0.0769 - acc: 0.9767\n",
      "Epoch 39/50\n",
      "90/90 [==============================] - 136s 2s/step - loss: 0.0629 - acc: 0.9763\n",
      "Epoch 40/50\n",
      "90/90 [==============================] - 126s 1s/step - loss: 0.0712 - acc: 0.9780\n",
      "Epoch 41/50\n",
      "90/90 [==============================] - 127s 1s/step - loss: 0.0647 - acc: 0.9812\n",
      "Epoch 42/50\n",
      "90/90 [==============================] - 129s 1s/step - loss: 0.0468 - acc: 0.9833\n",
      "Epoch 43/50\n",
      "90/90 [==============================] - 130s 1s/step - loss: 0.0440 - acc: 0.9833\n",
      "Epoch 44/50\n",
      "90/90 [==============================] - 131s 1s/step - loss: 0.0344 - acc: 0.9895\n",
      "Epoch 45/50\n",
      "90/90 [==============================] - 128s 1s/step - loss: 0.0427 - acc: 0.9843\n",
      "Epoch 46/50\n",
      "90/90 [==============================] - 127s 1s/step - loss: 0.0142 - acc: 0.9955\n",
      "Epoch 47/50\n",
      "90/90 [==============================] - 124s 1s/step - loss: 0.0421 - acc: 0.9875\n",
      "Epoch 48/50\n",
      "90/90 [==============================] - 99s 1s/step - loss: 0.0628 - acc: 0.9819\n",
      "Epoch 49/50\n",
      "90/90 [==============================] - 105s 1s/step - loss: 0.0550 - acc: 0.9847\n",
      "Epoch 50/50\n",
      "90/90 [==============================] - 125s 1s/step - loss: 0.0254 - acc: 0.9913\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_106 (Conv2D)         (None, 256, 256, 50)      500       \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 256, 256, 50)     200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_106 (MaxPooli  (None, 128, 128, 50)     0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_107 (Conv2D)         (None, 128, 128, 20)      9020      \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 128, 128, 20)     80        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_107 (MaxPooli  (None, 64, 64, 20)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_108 (Conv2D)         (None, 64, 64, 1)         181       \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 64, 64, 1)        4         \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_108 (MaxPooli  (None, 32, 32, 1)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " reshape_12 (Reshape)        (None, 32, 32)            0         \n",
      "                                                                 \n",
      " lstm_17 (LSTM)              (None, 128)               82432     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 80)                10320     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 4)                 324       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 103,061\n",
      "Trainable params: 102,919\n",
      "Non-trainable params: 142\n",
      "_________________________________________________________________\n",
      "13/13 [==============================] - 5s 344ms/step - loss: 3.6922 - acc: 0.7005\n",
      "Testing loss = 3.6922171115875244, Testing accuracy = 0.700507640838623\n"
     ]
    }
   ],
   "source": [
    "size_lstm = 128\n",
    "num_epochs = 50\n",
    "learning_rate = 0.005\n",
    "dense1_size = 80\n",
    "dense2_size = 4\n",
    "filter1_size = 50\n",
    "filter2_size = 20\n",
    "filter3_size = 1\n",
    "kernel_size = 3\n",
    "dropout_rate = 0.2\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(batch_input_shape=(None,img_height,img_width,1)))\n",
    "\n",
    "# CNN\n",
    "# Convolutional layer 1, batch normalization, max pooling\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters=filter1_size, \n",
    "    kernel_size=(kernel_size,kernel_size), \n",
    "    padding=\"same\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(padding=\"same\"))\n",
    "\n",
    "# Convolutional layer 2, batch normalization, max pooling\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters=filter2_size, \n",
    "    kernel_size=(kernel_size,kernel_size), \n",
    "    padding=\"same\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(padding=\"same\"))\n",
    "\n",
    "# Convolutional layer 3, batch normalization, max pooling\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters=filter3_size, \n",
    "    kernel_size=(kernel_size,kernel_size), \n",
    "    padding=\"same\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(padding=\"same\"))\n",
    "\n",
    "# LSTM\n",
    "model.add(tf.keras.layers.Reshape((32,32)))\n",
    "model.add(tf.keras.layers.LSTM(units=size_lstm))\n",
    "model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "model.add(tf.keras.layers.Dense(dense1_size, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "model.add(tf.keras.layers.Dense(dense2_size, activation='softmax'))\n",
    "\n",
    "# Train model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=num_epochs)\n",
    "model.summary()\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'Testing loss = {test_loss}, Testing accuracy = {test_acc}')   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42d3e2",
   "metadata": {},
   "source": [
    "# 2 class CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21f01218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "90/90 [==============================] - 128s 1s/step - loss: 0.3620 - acc: 0.8523\n",
      "Epoch 2/50\n",
      "90/90 [==============================] - 127s 1s/step - loss: 0.3029 - acc: 0.8787\n",
      "Epoch 3/50\n",
      "90/90 [==============================] - 126s 1s/step - loss: 0.2552 - acc: 0.9000\n",
      "Epoch 4/50\n",
      "90/90 [==============================] - 123s 1s/step - loss: 0.2515 - acc: 0.8944\n",
      "Epoch 5/50\n",
      "90/90 [==============================] - 123s 1s/step - loss: 0.2322 - acc: 0.9063\n",
      "Epoch 6/50\n",
      "90/90 [==============================] - 120s 1s/step - loss: 0.2448 - acc: 0.8990\n",
      "Epoch 7/50\n",
      "90/90 [==============================] - 117s 1s/step - loss: 0.1858 - acc: 0.9178\n",
      "Epoch 8/50\n",
      "90/90 [==============================] - 97s 1s/step - loss: 0.1661 - acc: 0.9348\n",
      "Epoch 9/50\n",
      "90/90 [==============================] - 99s 1s/step - loss: 0.1624 - acc: 0.9352\n",
      "Epoch 10/50\n",
      "90/90 [==============================] - 124s 1s/step - loss: 0.1372 - acc: 0.9446\n",
      "Epoch 11/50\n",
      "90/90 [==============================] - 127s 1s/step - loss: 0.1191 - acc: 0.9505\n",
      "Epoch 12/50\n",
      "90/90 [==============================] - 128s 1s/step - loss: 0.1005 - acc: 0.9554\n",
      "Epoch 13/50\n",
      "90/90 [==============================] - 123s 1s/step - loss: 0.1036 - acc: 0.9596\n",
      "Epoch 14/50\n",
      "90/90 [==============================] - 111s 1s/step - loss: 0.1054 - acc: 0.9519\n",
      "Epoch 15/50\n",
      "90/90 [==============================] - 99s 1s/step - loss: 0.1102 - acc: 0.9561\n",
      "Epoch 16/50\n",
      "90/90 [==============================] - 98s 1s/step - loss: 0.0779 - acc: 0.9659\n",
      "Epoch 17/50\n",
      "90/90 [==============================] - 99s 1s/step - loss: 0.0783 - acc: 0.9679\n",
      "Epoch 18/50\n",
      "90/90 [==============================] - 98s 1s/step - loss: 0.0812 - acc: 0.9690\n",
      "Epoch 19/50\n",
      "90/90 [==============================] - 99s 1s/step - loss: 0.1094 - acc: 0.9516\n",
      "Epoch 20/50\n",
      "90/90 [==============================] - 98s 1s/step - loss: 0.0820 - acc: 0.9631\n",
      "Epoch 21/50\n",
      "90/90 [==============================] - 102s 1s/step - loss: 0.0750 - acc: 0.9714\n",
      "Epoch 22/50\n",
      "90/90 [==============================] - 98s 1s/step - loss: 0.0614 - acc: 0.9756\n",
      "Epoch 23/50\n",
      "90/90 [==============================] - 105s 1s/step - loss: 0.0516 - acc: 0.9794\n",
      "Epoch 24/50\n",
      "90/90 [==============================] - 122s 1s/step - loss: 0.0482 - acc: 0.9815\n",
      "Epoch 25/50\n",
      "90/90 [==============================] - 130s 1s/step - loss: 0.0498 - acc: 0.9815\n",
      "Epoch 26/50\n",
      "90/90 [==============================] - 134s 1s/step - loss: 0.0557 - acc: 0.9812\n",
      "Epoch 27/50\n",
      "90/90 [==============================] - 130s 1s/step - loss: 0.0453 - acc: 0.9826\n",
      "Epoch 28/50\n",
      "90/90 [==============================] - 125s 1s/step - loss: 0.0427 - acc: 0.9861\n",
      "Epoch 29/50\n",
      "90/90 [==============================] - 123s 1s/step - loss: 0.0449 - acc: 0.9833\n",
      "Epoch 30/50\n",
      "90/90 [==============================] - 107s 1s/step - loss: 0.0451 - acc: 0.9847\n",
      "Epoch 31/50\n",
      "90/90 [==============================] - 94s 1s/step - loss: 0.0687 - acc: 0.9760\n",
      "Epoch 32/50\n",
      "90/90 [==============================] - 94s 1s/step - loss: 0.0585 - acc: 0.9774\n",
      "Epoch 33/50\n",
      "90/90 [==============================] - 94s 1s/step - loss: 0.0406 - acc: 0.9847\n",
      "Epoch 34/50\n",
      "90/90 [==============================] - 95s 1s/step - loss: 0.0301 - acc: 0.9916\n",
      "Epoch 35/50\n",
      "90/90 [==============================] - 96s 1s/step - loss: 0.0272 - acc: 0.9899\n",
      "Epoch 36/50\n",
      "90/90 [==============================] - 106s 1s/step - loss: 0.0189 - acc: 0.9948\n",
      "Epoch 37/50\n",
      "90/90 [==============================] - 115s 1s/step - loss: 0.0257 - acc: 0.9913\n",
      "Epoch 38/50\n",
      "90/90 [==============================] - 116s 1s/step - loss: 0.0083 - acc: 0.9972\n",
      "Epoch 39/50\n",
      "90/90 [==============================] - 107s 1s/step - loss: 0.0117 - acc: 0.9962\n",
      "Epoch 40/50\n",
      "90/90 [==============================] - 118s 1s/step - loss: 0.0394 - acc: 0.9882\n",
      "Epoch 41/50\n",
      "90/90 [==============================] - 117s 1s/step - loss: 0.0289 - acc: 0.9920\n",
      "Epoch 42/50\n",
      "90/90 [==============================] - 118s 1s/step - loss: 0.0070 - acc: 0.9972\n",
      "Epoch 43/50\n",
      "90/90 [==============================] - 124s 1s/step - loss: 0.0339 - acc: 0.9892\n",
      "Epoch 44/50\n",
      "90/90 [==============================] - 107s 1s/step - loss: 0.0398 - acc: 0.9864\n",
      "Epoch 45/50\n",
      "90/90 [==============================] - 98s 1s/step - loss: 0.0126 - acc: 0.9965\n",
      "Epoch 46/50\n",
      "90/90 [==============================] - 106s 1s/step - loss: 0.0213 - acc: 0.9920\n",
      "Epoch 47/50\n",
      "90/90 [==============================] - 109s 1s/step - loss: 0.0232 - acc: 0.9927\n",
      "Epoch 48/50\n",
      "90/90 [==============================] - 97s 1s/step - loss: 0.0238 - acc: 0.9934\n",
      "Epoch 49/50\n",
      "90/90 [==============================] - 94s 1s/step - loss: 0.0161 - acc: 0.9937\n",
      "Epoch 50/50\n",
      "90/90 [==============================] - 92s 1s/step - loss: 0.0243 - acc: 0.9944\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 256, 256, 50)      500       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 256, 256, 50)     200       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 128, 128, 50)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 128, 128, 20)      9020      \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 128, 128, 20)     80        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 64, 64, 20)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 64, 64, 1)         181       \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 64, 64, 1)        4         \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 32, 32, 1)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 32, 32)            0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 128)               82432     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 80)                10320     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 162       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102,899\n",
      "Trainable params: 102,757\n",
      "Non-trainable params: 142\n",
      "_________________________________________________________________\n",
      "13/13 [==============================] - 4s 237ms/step - loss: 0.4467 - acc: 0.8528\n",
      "Testing loss = 0.44666117429733276, Testing accuracy = 0.8527919054031372\n"
     ]
    }
   ],
   "source": [
    "size_lstm = 128\n",
    "num_epochs = 50\n",
    "learning_rate = 0.005\n",
    "dense1_size = 80\n",
    "dense2_size = 2\n",
    "filter1_size = 50\n",
    "filter2_size = 20\n",
    "filter3_size = 1\n",
    "kernel_size = 3\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(batch_input_shape=(None,img_height,img_width,1)))\n",
    "\n",
    "# CNN\n",
    "# Convolutional layer 1, batch normalization, max pooling\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters=filter1_size, \n",
    "    kernel_size=(kernel_size,kernel_size), \n",
    "    padding=\"same\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(padding=\"same\"))\n",
    "\n",
    "# Convolutional layer 2, batch normalization, max pooling\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters=filter2_size, \n",
    "    kernel_size=(kernel_size,kernel_size), \n",
    "    padding=\"same\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(padding=\"same\"))\n",
    "\n",
    "# Convolutional layer 3, batch normalization, max pooling\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters=filter3_size, \n",
    "    kernel_size=(kernel_size,kernel_size), \n",
    "    padding=\"same\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(padding=\"same\"))\n",
    "\n",
    "# LSTM\n",
    "model.add(tf.keras.layers.Reshape((32,32)))\n",
    "model.add(tf.keras.layers.LSTM(units=size_lstm))\n",
    "model.add(tf.keras.layers.Dropout(dropout_rate ))\n",
    "model.add(tf.keras.layers.Dense(dense1_size, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(dropout_rate ))\n",
    "model.add(tf.keras.layers.Dense(dense2_size, activation='softmax'))\n",
    "\n",
    "# Train model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['acc'])\n",
    "\n",
    "model.fit(x_train2, y_train2, epochs=num_epochs)\n",
    "model.summary()\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(x_test2, y_test2)\n",
    "\n",
    "print(f'Testing loss = {test_loss}, Testing accuracy = {test_acc}')   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroDet_env",
   "language": "python",
   "name": "neurodet_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
